---
title: "Final Project Part 1"
author: "Zuhayr Baqar"
date: "5/27/2021"
output: html_document
---

---
title: "Final Project: Structure & Deliverables"
author: "Zuhayr Baqar, Dan Ngyuyen, Rachel Roggenkemper, Annie Zell (Group 5)"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
```

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readxl)
library(kableExtra)
library(extrafont)
library(broom)
library(extrafont)
library(gapminder)
library(transformr)
library(gganimate)
library(plotly)
library(gifski)


```

# Introduction
Growing up in the United States, the mindset of the United States being the best country in the world was one I would often happen upon. But as the years go by, this question of the best country becomes deeper and deeper. One aspect of understanding this question of “best country” is knowing what variables are considered. Some are quality of life, healthcare, economic and political stability, but in addition to these, one of the most important is a country’s education system. Today, we will take a deeper dive into the United States’ education system. The data we will be using was pulled from the Urban Institute’s API. The dataset brings together multiple facets of U.S. education data, including data related to a student’s race, sex, expenditures, and test scores throughout the years.
```{r}
US_schools <- read_excel("US_schools_data.xlsx")
```

# Data Cleaning

Before we get into analyzing the data, let’s quickly discuss the steps we took into cleaning our dataset. Since this dataset is a collection of multiple datasets that have been merged together, we had our work cut out for us in regards to data cleaning. 
The first step we took was narrowing our data, primarily focusing on the instruction (education) expenditure variables, testing variables (whether it is reading or math test, and the average score), the grade of the students (4 or 8 in this dataset),  state, and year. 
Next, since the data was given to us separated by states, we wanted to focus more on regions. We decided to split up our regions geographically; the geographic regions we split our states into include: West, Midwest, Southwest, Southeast, and Northeast. We believed these regions would have differing relationships between instruction expenditures and average test scores because we hypothesized that the states located within the regions might have some economical and geographical differences. We thought that if we grouped these similar regions together, when we compare the five regions amongst each other, we would see a difference in relationships between school expenditures and student test scores.

```{r}
#Split the year from the state
year_state<- US_schools$PRIMARY_KEY
x <- str_replace_all(year_state, "_", " ")
x <- str_split_fixed(x, " ", n = 2)

split_year_state <- data.frame(x)
split_year_state<- rename(split_year_state, YEAR = X1, STATE = X2)
  
#select columns and merge split_year_state columns
schools_clean <- US_schools %>%
  cbind(split_year_state) %>%
  select(YEAR, STATE,TOTAL_EXPENDITURE, INSTRUCTION_EXPENDITURE, SUPPORT_SERVICES_EXPENDITURE, OTHER_EXPENDITURE, CAPITAL_OUTLAY_EXPENDITURE, ends_with(c("READING", "MATHEMATICS")) )
  
```




```{r}
schools_clean <- schools_clean %>%
  pivot_longer(8:47, names_to = c("GRADE", "RACE", "SEX", "TEST"), names_sep ="_", values_to = "TEST_SCORES")
```




```{r, message = FALSE, warning = FALSE, include = FALSE}
# creating a data frame called regions which contains each State included in the min_wage dataset and its corresponding region 
regions <- 
  data.frame(
    STATE = toupper(c("MAINE", "MASSACHUSETTS", "Rhode Island", "Connecticut", "New Hampshire", "Vermont", "New York", "Pennsylvania", "New Jersey", "Delaware", "Maryland", "West Virginia", "Virginia", "Kentucky", "Tennessee", "North Carolina", "South Carolina", "Georgia", "Alabama", "Mississippi", "Arkansas", "Louisiana", "Florida", "Ohio", "Indiana", "Michigan", "Illinois", "Missouri", "Wisconsin", "Minnesota", "Iowa", "Kansas", "Nebraska", "South Dakota", "North Dakota", "Texas", "Oklahoma", "New Mexico", "Arizona", "Colorado", "Wyoming", "Montana", "Idaho", "Washington", "Oregon", "Utah", "Nevada", "California", "Alaska", "Hawaii", "District of Columbia", "Federal (FLSA)", "U.S. Virgin Islands", "Guam", "Puerto Rico")), 
    Region = c("Northeast", "Northeast", "Northeast", "Northeast", "Northeast", "Northeast", "Northeast", "Northeast", "Northeast", "Northeast", "Northeast", "Southeast", "Southeast", "Southeast", "Southeast", "Southeast", "Southeast", "Southeast", "Southeast", "Southeast", "Southeast", "Southeast", "Southeast", "Midwest", "Midwest", "Midwest", "Midwest", "Midwest", "Midwest", "Midwest", "Midwest", "Midwest", "Midwest", "Midwest", "Midwest", "Southwest", "Southwest", "Southwest", "Southwest", "West", "West", "West", "West", "West", "West", "West", "West", "West", "West", "West", "Southeast", "Other", "Other", "Other", "Other"))
```

```{r}
schools_clean <- 
  schools_clean %>%
  left_join(regions, by = "STATE") %>%
  filter(STATE != "DODEA", STATE != "NATIONAL")
```




```{r}

sex_fct <- schools_clean$SEX

sex_fct <- fct_recode(sex_fct, All = "A", Male = "M", Female = "F")

schools_clean <- schools_clean %>% mutate(SEX = sex_fct)

```

```{r}
schools_clean <- schools_clean %>% mutate(GRADE = str_extract(GRADE, "[1-9]"))
```

```{r}
schools_clean$RACE[schools_clean$RACE == "AM"] = "American Indian or Alaska Native"
schools_clean$RACE[schools_clean$RACE == "AS"] = "Asian"
schools_clean$RACE[schools_clean$RACE == "HI"] = "Hispanic/Latino"
schools_clean$RACE[schools_clean$RACE == "BL"] = "Black or African American"
schools_clean$RACE[schools_clean$RACE == "WH"] = "White"
schools_clean$RACE[schools_clean$RACE == "HP"] = "Hawaiian Native/Pacific Islander"
schools_clean$RACE[schools_clean$RACE == "TR"] = "Two or More Races"
schools_clean$RACE[schools_clean$RACE == "A"] = "All"
```

```{r}
# creating a dataset that removes na values to match the number of observations with the regressions.
schools_model <- schools_clean %>% 
  filter(!is.na(TEST_SCORES)) %>% 
  filter(!is.na(INSTRUCTION_EXPENDITURE)) %>% 
  filter(!is.na(YEAR))
```

# Data Visualizations

## Figure 1

```{r}


plot1 <- schools_clean %>% 
  ggplot(aes(x = INSTRUCTION_EXPENDITURE, y = TEST_SCORES)) + 
  geom_jitter(cex = .8, aes(color = RACE), alpha = 0.5) + 
  scale_fill_brewer() +
  geom_smooth(color = "black",fill = "darkgrey", method = "lm") +
  facet_wrap(~Region, nrow =  3)+
  labs(x = "Instruction Expenditure", y = "Test Scores", title = "How School Expenditures Impacts Test Scores") +
  ggtitle("How School Expenditures Impacts Test Scores") +
  xlab("Instruction Expenditure") + ylab("Test Scores") +
  theme(text=element_text(size=10, family="serif"), legend.position = "right") 

plot1 + scale_colour_brewer(palette = "Set2") 
```

This above plot shows the relationship between instructional expenditures and testing scores, how this differs by region, in addition to taking into account both sex and race. One aspect of the data set that the above plot reveals right away is a “hole” in the data collection process, more specifically when taking into account race and sex. When the sex is specified (male or female), we no longer have data about the corresponding race, we only have the grouped “All” race. And likewise, when race is specified, we no longer know the corresponding sex, we only have the grouped “All” sex. This is why the first line of plots plotting the “All” variable of sex is the only one that shows the different colors of Race. To further analyze this plot, we see that the relationship between instructional expenditures and testing scores differs by regions, but does not differ greatly by sex. We know this because the regression line between the different regions are very different, whereas the regression line for the same region and different sex look very similar. We can see that the Midwest and Southeast have the lowest amount of instruction expenditures, because there are no data points that go above the 20 million mark. In addition, we can see that the Midwest, Northeast, and West have fairly flat and horizontal regression lines, meaning there is little to no relationship between instructional expenditures and testing scores. However, we see a positive relationship in the Southeast; as instruction expenditure increases, so do testing scores. 

## Figure 2

```{r, message = FALSE, warning = FALSE}
cdPalette <- c("#F1c2bc", "#Ffd5b9", "#Ebffb9", "#B9ffd6", "#B9fff8", "#B9bcff", "#Ffb9fe", "#C6bcbc")
loadfonts(device = "win")
schools_clean %>%
  ggplot(aes(x = TEST_SCORES, fill = RACE)) + 
  geom_boxplot() + 
  facet_wrap(SEX~TEST, nrow = 3) + 
  theme(panel.spacing = unit(0.1, "lines")) + 
  theme(panel.grid.minor = element_line(colour = "white", size = 1)) +
  labs(x = "Test Scores", title = "Distribution of Mathematics and Reading Test Scores by Sex and Race", y = "") +
  # annotate("text", x = 150, y = -0.33, label = "All", color = "black", size = 3) +
  scale_fill_manual(values = cdPalette) +
  theme(text = element_text(size = 12,  family = "serif")) +
  theme(axis.ticks = element_blank(), axis.text.y = element_blank())
```

This above plot shows the distribution of mathematics and reading test scores, in addition to taking into account both sex and race. This plot shows the similar “hole” in the data as the above plot, where Race is only specifically defined when Sex is not speicially defined, and vice versa. The distribution of mathematics and reading test scores displays that the mathematics scores tend to be slightly higher than the reading test scores throughout the different groups and facets. Additionally, it appears that females tend to score slightly higher on the reading test whereas males tend to score slightly higher on the mathematics test. To take a look at race, it appears that people who identify as Asian tend to have the highest reading and mathematics test scores. Next, it appears that people who identify as “White” have the second highest test scores. After ranking based on the test scores, comes people who identify as “Two or More Races”, then “Hispanic / Latino”, and after “Hawaiian Native / Pacific Islander”, “Black or African American”, and “American Indian or Alaska Native” all score very similarly. 

## Figure 3

```{r}
Expend_Test <- schools_model %>% 
  filter(!is.na(TEST_SCORES)) %>%
  group_by(YEAR) %>%
  ggplot(aes(x = INSTRUCTION_EXPENDITURE, y = TEST_SCORES, color = "red", linetype = GRADE)) + 
  geom_jitter(cex = .8, show.legend = FALSE) + 
  facet_wrap(~Region, nrow =  3)+
  labs(x = "Instruction Expenditure ($)", y = "Test Scores", title = "How Instruction Expenditures Impacts Test Scores Pver Time") +
  labs(title = 'Year: {frame_time}', x = 'Instruction Expenditure', y = 'Test Scores') +
  transition_time(as.integer(YEAR)) +
  ease_aes('linear') + theme(axis.text.x = element_text(size = 7, angle = 90)) + 
  scale_x_continuous(labels = scales::comma)
animate(Expend_Test)
```

Figure 3 shows how the relationship between instructional expenditures and test scores change over time in five regions within the United States. All 5 regions have lower instruction expenditures and test scores throughout the 90s. In the 2000s a gap between high and low instruction expenditures developed, especially in the West, Southwest and Northeast, while the Midwest and Southeast maintained relatively low instruction expenditures. Although all regions demonstrate an increase in the distribution of test scores over time, only the Southeast and Southwest demonstrated a slight positive relationship between school expenditures and test scores. 

## Figure 4

```{r}
p <- schools_clean %>%
  filter(!is.na(TEST_SCORES)) %>%
  filter(!is.na(INSTRUCTION_EXPENDITURE)) %>%
  filter(!is.na(YEAR)) %>%
  ggplot(aes(y = TEST_SCORES, x = Region)) + 
  geom_boxplot(aes(fill = Region, alpha = .6), show.legend = FALSE) +
  scale_fill_brewer() +
  geom_jitter(alpha = 0.7, cex = .4, color = "red") +
  transition_states(as.integer(YEAR),transition_length = 2, state_length = 1 ) +
  enter_fade() + 
  exit_shrink() +
  ease_aes('sine-in-out') +
  labs(title = "How US Test Scores Change Over Time, Regionally", subtitle = 'Year: {frame_time}', x = "", y = "Test Scores") + 
  theme(text = element_text(size=20)) +
  transition_time(as.integer(YEAR)) 

p
  
animate(p, fps = 10, width = 750, height = 450)
anim_save("plot2.gif")

```

Figure 4 shows how the distribution of test scores changes over time by region. This visualization also shows how more data was collected overtime. Starting around 2005 there are a lot more data points for each region, which may explain the increase in variation. Overall the distribution does not change the average test score too much, but there does appear to be a gradual increase in test scores over time, with the Northeast leading the way. 

How school instructional expenditures and test scores have changed over time, and how this differs by region

Each of these visualizations should also investigate how these relationships differ based on student race and sex.

# Linear Regression

To begin with, we created two simple models  to analyze if there is any difference between the mathematics and reading tests when it came to predicting test scores from instructions expenditure. To measure the accuracy of the models, we decided to use the R-squared statistic, which measures the variability accounted for in the response variable (test scores) due to the explanatory variable (instructional expenditure).
```{r LinReg}
expend_math <- schools_clean %>% filter(TEST == "MATHEMATICS") %>% lm(TEST_SCORES ~ INSTRUCTION_EXPENDITURE, data = .)

expend_reading <- schools_clean %>% filter(TEST == "READING") %>% lm(TEST_SCORES ~ INSTRUCTION_EXPENDITURE, data = .)

rsq <- c()

```

From the two models that we created, the math model resulted in instruction expenditure accounting for 0.426% of the variability in test scores while the reading model resulted in instruction expenditure accounting for 0.036% of the variability in test scores. This means that the model is slightly more accurate in predicting math test scores given instructional expenditure. That being said, the variability accounted for is still less than 1%. In order to generate a more accurate model, we checked the R-squared as we regressed on more explanatory variables. 
## Model Comparison
For which test does instructional expenditures account for a larger proportion of the variability? How substantial is the difference?


### Relationship between instructional expenditures and *mathematics* test scores 
```{r MathReg}
rsq <- append(rsq, (summary(expend_math) %>% glance() %>% select(r.squared) ))
```

### Relationship between instructional expenditures and *reading* test scores 

```{r ReadingReg}
rsq <- append(rsq, summary(expend_reading) %>% glance() %>% select(r.squared) )
```



## Multiple Linear Regression
### Both Tests

To account for the test, instead of creating two separate models, we incorporated the *test* variable as a categorical predictor in the equation. We also believed that the *year* of the observation would be important, since we imagined that expenditure would increase in tandem with inflation and the overall standard of living in the United States, which has increased over the years. After adding these two variables into our model, our R-squared had increased to 0.1569, or 15.69% of variability in the test scores accounted for. Better, but still not ideal.

```{r MLR}
rsq <- append(rsq, schools_clean  %>% 
  lm(
    TEST_SCORES ~ INSTRUCTION_EXPENDITURE + TEST, data = .
    ) %>% 
  summary() %>% glance() %>% select(r.squared))
```

To account for the test, instead of creating two separate models, we incorporated the *test* variable as a categorical predictor in the equation. We also believed that the *year* of the observation would be important, since we imagined that expenditure would increase in tandem with inflation and the overall standard of living in the United States. After adding these two variables into our model, our R-squared had increased to 0.1569, or 15.69% of variability in the test scores accounted for. Better, but still not ideal.

```{r}
rsq <- append(rsq, schools_clean  %>% mutate(YEAR = as.numeric(YEAR)) %>% 
  lm(TEST_SCORES ~ INSTRUCTION_EXPENDITURE + TEST + YEAR  , data = .) %>%
  summary() %>% glance() %>% select(r.squared))
```
## Adjusting for Complexity
There is a trade off between model “complexity” and an increase in a model’s R2. Unfortunately, even if a variable doesn’t add much to the model, the R2 for that model will still increase. So, we need a different measure that can account for whether the variable(s) explain components of the variability in test scores that weren’t accounted for by other variables.

This is where adjusted R2 comes in. By “adjusting” the R2, we are essentially making a penalty for whether the extra variable added something “new” to the model.

Include additional variables in your regression and see how much variability in test scores your model can account for. Use adjusted R2 to decide on what final model your group believes is the “best.”

Again, we decided to add some seemingly relavent variables to our model to see if it improved our R-squared. One of the variables was *GRADE* because we had a hypothesis that the grade in which the student in presumably affects their tests score. More specifically, we thought that a higher grade would result in a higher test score, since the student would have more knowledge and therefore more capable of achieving a better score. The other variable we accounted for is *Region*. Different regions most likely spend different amounts of money on education, so we wanted to have that reflected in our model. Also by including regions, we could potentially see which regions were scoring lower or higher on the tests, and see if there was any correlation between the region and the test score. After adding these two variables (*GRADE* and *Region*), our R-squared was now 0.8464, meaning that almost 85% of the variability in the observed test scores was being accounted for by the explanatory variables we included in our model. This is much better, and now some deductions could be made.
```{r}
final_model <- schools_clean  %>% 
  mutate(YEAR = as.numeric(YEAR)) %>% 
  lm(
    TEST_SCORES ~ INSTRUCTION_EXPENDITURE + TEST + YEAR + GRADE + Region , data = .
    )
rsq <- append(rsq, summary(final_model) %>% glance() %>% select(r.squared))
```

With your final model, make a visualization that explores the relationships accounted for in your model.

```{r}
RSquared <- unlist(rsq)
Y_Variable <- c("Test Score", "Test Score", "Test Score", "Test Score", "Test Score")
X_Variable <- c("Ed. Expenditure (Math)", "Ed. Expenditure (Reading)", "Ed. Expenditure, Test", "Ed. Expenditure, Test, Year", "Ed. Expenditure, Test, Year, Grade, Region")
rsq_models <- data.frame(RSquared, Y_Variable, X_Variable)
```

```{r}
rsq_models
```


```{r}
plot1 <- schools_model %>% 
  filter(!is.na(TEST_SCORES)) %>%
  group_by(YEAR) %>%
  ggplot(aes(x = INSTRUCTION_EXPENDITURE, y = TEST_SCORES, color = TEST, linetype = GRADE)) + 
  geom_smooth(method = "lm") +
  geom_jitter(cex = .8, show.legend = FALSE) + 
  scale_fill_brewer() +
  facet_wrap(~Region, nrow =  3)+
  labs(x = "Instruction Expenditure", y = "Test Scores", title = "How School Expenditures Impacts Test Scores") +
  labs(title = 'Year: {frame_time}', x = 'Instruction Expenditure', y = 'Test Scores') +
  transition_time(as.integer(YEAR)) +
  ease_aes('linear') + theme(axis.text.x = element_text(size = 7, angle = 90)) + 
  scale_x_continuous(labels = scales::comma)


animate(plot1 + scale_colour_brewer(palette = "Set2"))
```

With your chosen model, generate predictions using the predict() function. Then, add random errors to the predictions, using the residual standard error estimated from the linear regression model (acquired with sigma()).

```{r}
# saving the predicted values and the standard deviation
model_pred <- predict(final_model)
model_sig <- sigma(final_model)
```

Tip: Measure the R-squared between expected vs observed values, SSE, and RMSE



```{r}
noise <- function(x, mean = 0, sd){
  n <- length(x)
  new_data <- x + rnorm(n, mean, sd)
  return(new_data)
}
```

```{r}
schools_model <- schools_model %>% 
  mutate(
    predicted = noise(model_pred, sd = model_sig)
                   )
```

Now, compare these simulated observations to the observed data. Generate the same plot of the relationships modeled by the linear regression, for the simulated data (that you made at the end of Part One).
Plot the visualization of the observed data and the simulated data side-by-side. Discuss how the simulated data are similar and / or different to the observed data.

```{r}
obs_plot  <- schools_model %>% 
  filter(!is.na(TEST_SCORES)) %>%
  group_by(YEAR) %>%
  ggplot(aes(x = INSTRUCTION_EXPENDITURE, y = TEST_SCORES, color = TEST, linetype = GRADE)) + 
  geom_smooth(method = "lm") +
  geom_jitter(cex = .8, show.legend = FALSE) + 
  scale_fill_brewer() +
  facet_wrap(~Region, nrow =  3)+
  labs(x = "Instruction Expenditure", y = "Test Scores", title = "How School Expenditures Impacts Test Scores") +
  labs(title = 'Year: {frame_time}', x = 'Instruction Expenditure', y = 'Observed Test Scores') +
  transition_time(as.integer(YEAR)) +
  ease_aes('linear') + theme(axis.text.x = element_text(size = 7, angle = 90)) + 
  scale_x_continuous(labels = scales::comma)


animate(obs_plot + scale_colour_brewer(palette = "Set2"))
```

We performed a predictive check for our chosen linear model to see if the assumed linear model accurately describes the observed data. We created a visualization that shows the distribution of test scores and instructional expenditures for both predicted and observed values. In general, these two models are pretty similar with very few noticeable differences. From 2011-2015, it seemed that observed values had test scores higher than predicted values for reading. In addition, for 1992, 2000, and 2002, it seemed that predicted values had more data with lower instructional expenditures equally distributed between test scores than observed values. Overall, predicted values didn’t seem to be drastically different from observed values. 

```{r}
pred_plot  <- schools_model %>% 
  filter(!is.na(TEST_SCORES)) %>%
  group_by(YEAR) %>%
  ggplot(aes(x = INSTRUCTION_EXPENDITURE, y = predicted, color = TEST, linetype = GRADE)) + 
  geom_smooth(method = "lm") +
  geom_jitter(cex = .8, show.legend = FALSE) + 
  scale_fill_brewer() +
  facet_wrap(~Region, nrow =  3)+
  labs(x = "Instruction Expenditure", y = "Test Scores", title = "How School Expenditures Impacts Test Scores") +
  labs(title = 'Year: {frame_time}', x = 'Instruction Expenditure', y = 'Predicted Test Scores') +
  transition_time(as.integer(YEAR)) +
  ease_aes('linear') + theme(axis.text.x = element_text(size = 7, angle = 90)) + 
  scale_x_continuous(labels = scales::comma)


animate(pred_plot + scale_colour_brewer(palette = "Set2"))


```


```{r}
nsims <- 1000

sims <- map_dfc(1:nsims,
                ~tibble(sim = noise(model_pred, sd = model_sig))) 
sims <- schools_model %>% 
  select(TEST_SCORES) %>% 
  bind_cols(sims)
```
```{r}
obs_vs_sim <- function(df){
  lm(schools_model$TEST_SCORES ~ x)
}

sim_r_sq <- sims %>% 
  map( ~lm(TEST_SCORES ~ .x, data = sims)) %>% 
  map(glance) %>% 
  map_dbl(~.$r.squared)

```

```{r}
sim_r_sq <- as.data.frame(sim_r_sq)
```

```{r}
sim_r_sq %>% ggplot(aes(sim_r_sq)) + geom_histogram(color = "green", fill = "dark blue") + xlim(0.7, 0.73) + labs(x = "Simulated R-squared", y = "Count")
```

Finally, we generated 1000 simulated datasets, regressed the observed dataset against each simulated dataset, and kept the r^2 from the regression. We then plotted the distribution of the r^2 values from the regression. In the plot, we noticed that the r^2 values from the simulated dataset ranged from 0.707 to 0.73. With this range, we can conclude that the data we simulated under our statistical model is strongly similar to the observed values. This means that our statistical model does not account for about 28% of the observed variability in the observed test scores. 
